# -*- coding: utf-8 -*-
# -*- mode: org -*-

#+TITLE: Assessing the Computation and Communication Overhead of Operating System Containers for HPC Applications
#+AUTHOR: Lucas Mello Schnorr

#+STARTUP: overview indent
#+LANGUAGE: pt-br
#+OPTIONS: H:3 creator:nil timestamp:nil skip:nil toc:nil num:t ^:nil ~:~
#+OPTIONS: author:nil title:nil date:nil
#+TAGS: noexport(n) deprecated(d) ignore(i)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [12pt]
#+LATEX_HEADER: \usepackage{sbc-template}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[T1]{fontenc}

# You need at least Org 9 and Emacs 24 to make this work.
# If you do, just type make (thanks Luka Stanisic for this).

* IEEETran configuration for org export + ignore tag (Start Here)  :noexport:

#+begin_src emacs-lisp :results output :session :exports both
(add-to-list 'load-path ".")
(require 'ox-extra)
(ox-extras-activate '(ignore-headlines))
#+end_src

#+RESULTS:

* *The Paper*                                                       :ignore:
** Latex configurations                                             :ignore:

#+BEGIN_EXPORT latex
%\usepackage[brazil]{babel}   
\sloppy
#+END_EXPORT

** Frontpage                                                        :ignore:

#+BEGIN_EXPORT latex
\title{Assessing the Computation and Communication Overhead of Operating System Containers for HPC Applications}

\author{
   Guilherme Rezende Alles\inst{1},
   Alexandre Carissimi\inst{1},
   Lucas Mello Schnorr\inst{1}}

\address{
   Graduate Program in Computer Science (PPGC/UFRGS), Porto Alegre, Brazil}
#+END_EXPORT

#+LaTeX: \maketitle

** Abstract                                                         :ignore:

#+LaTeX: \begin{abstract}

This paper explores the use of container technologies to solve two relevant problems in high-performance computing (HPC) applications. The first problem is reproducibility, expressed by the difficulty of precisely reproducing an execution environment to verify, validate or build on previous work. The second problem relates to the lack of flexibility of traditional HPC resources such as clusters and grids upon its software stack, which prevents users to easily configure an execution environment with arbitrary software such as specific versions of libraries or drivers.

#+LaTeX: \end{abstract}

** Introduction

One fundamental aspect of scientific computing research is that, in order for it to be credible, it should be reproducible. This means that the researcher has the responsibility of providing its readers with the resources necessary to run experiments and obtain similar results to the ones provided by the research. Although the concept of reproducibility is well known in the scientific community, it is very hard to put it to practice because of the large number of variables that comprise an execution environment. Additionally, HPC resources are usually managed under strict usage policies, many of which do not allow the user to customize the environment with an arbitrary software stack (e.g. compilers, libraries or device drivers), thus making it very difficult to obtain a uniform execution platform across different clusters.

Although being intuitively difficult to deal with, these problems can be solved through the use of traditional hardware virtualization. By abstracting the hardware layer and providing a generic, well-known API to the software stack, users can run their own operating system and execution environment on top of a hypervisor, ensuring that the virtual environment can be configured to the exact requirements for a given experiment. The problem with this approach is that, when considering HPC applications, the performance overhead introduced by the hypervisor and an entire virtual machine is too high for this solution to be considered feasible.

Lately, container technologies have gained a lot of attention in the software development industry. This is mainly because containers are presented as a solution to package enterprise software in a controlled, static environment that will run on a wide variety of Linux systems. When compared to traditional virtual machines, containers have the advantage of not needing a hypervisor for virtualization, yielding performance that is very close to native while still providing application and software stack isolation. 

By using containers to isolate and package software, users are able to create reproducible, configurable environments for their applications, thus diminishing the hassle of managing application dependencies and software stack differences when executing experiments across different Linux environments.

In fact, this approach has already been used to great extents in the software development industry. Container technologies such as Docker are used daily to deploy software microservices to cloud environments, and cloud infrastructure providers such as AWS and Google Cloud Platform offer services to manage applications based on containers.

Considering the large adoption of containers in software development, we compare and contrast two container technologies, Docker and Singularity, with respect to their performance overhead and ability to solve common HPC workflow problems. For the performance measurements, we will consider the same experiments running on a native environment as the baseline.

** Related Work and Motivation
*** TODO Related Work
 - */Citations are not showing up. Still have to figure this out.../*
As presented in \cite{7562612}, the virtualization of HPC applications is viable in a low overhead environment such as Docker. In this study, the author compares the compute performance of Docker containers to virtual machines, concluding that the former has a considerably lower overhead when compared to the latter. This study, however, only explores the performance of single node applications, and does not present information on how containers can scale to distributed environments which are usually the case for high-performance applications.

For multi-node computations, \cite{7868429} proposes the creation of an MPI cluster using Docker containers in a distributed environment. In this proposal, the containers are connected through an orchestrator called Docker Swarm, which is responsible for assigning names and providing an overlay network for transparent connectivity between the containers. Performance analysis, however, is absent from this study, obscuring the conclusion of whether such an approach is viable in a real-world scenario.

In \cite{10.1371/journal.pone.0177459}, the author introduces Singularity. Singularity is a container system designed for scientific research workflows, and it strives to solve some drawbacks of using Docker containers in HPC. The author discusses how Docker was not designed for shared, multi-user environments (such as supercomputers and HPC centers) and thus presents significant security issues when used in this context. As a consequence, it is very hard to find HPC centers that allow users to execute Docker containers. Singularity, on the other hand, is built with these problems in mind and solves them in order to make HPC containers accessible to the scientific community. As a consequence, Singularity containers are already accepted and used in many supercomputers around the world. Additionally, \cite{Le:2017:PAA:3093338.3106737} presents some performance analysis of applications running on top of Singularity containers. It concludes that while some overhead does exist, the reported values are negligible for most use cases. 

*** Motivation
Our objectives for this work is to study the drawbacks and improvements that occur by applying virtualization techniques to high-performance computing workflows. As concluded by previous work, using virtual machines is not a feasible approach because of the performance and communication overhead that comes along with this strategy. Thus, our goal is to measure the performance impact of applying virtualization in the form of container technologies to these workloads. We present an analysis covering both synthetic benchmarks and a real application comparing the usability of two major container systems - Docker and Singularity - using a traditional approach (with no virtualization) as a baseline.

Furthermore, we intend to demonstrate that virtualization techniques can be used in HPC without the massive overhead of traditional virtual machines. By using containers, cluster administrators can provide flexibility, portability and enhanced reproducibility to its users without sacrificing performance and security.

** Background and Experimental Context
*** Background
**** Containers
Containers are a mean of achieving virtualization without relying on software to emulate hardware resources. Instead, containers are known as software level virtualization for Linux systems, and they use features that are native to the Linux kernel (namely, \textit{cgroups} and \textit{namespaces}) to isolate the resources managed by the operating system. As a result, software that runs inside of a container can have its own file system, process tree, user space and network stack, giving it the impression of being executed on a completely isolated environment.

By using native kernel features to grant isolation, containers present a theoretically negligible overhead penalty when compared to an application running natively on the host operating system. This happens because the Linux kernel already uses \textit{cgroups} and \textit{namespaces} to manage its resources internally, even when there are not multiple containers on a single machine. Considering this approach, a non-virtualized Linux environment can be seen itself as a single container running on top of the Linux kernel, which means that there is no additional software layer in a container to insert execution overhead.

In spite of being receiving large amounts of attention lately, the core APIs and functionality used to create containers is not new, and have been present in the Linux kernel for more than a decade. However, the popularization of containers took a long time to happen especially because of how difficult it is for an end user to interact with these kernel APIs directly. Conversely, containers only became popular when software (such as Docker and Singularity) was created to interact with the kernel and mediate the creation of containers.

These container management platforms also introduced new features which were very desirable for many workflows (including software development and HPC), such as the ability to encapsulate an entire environment in an image that can be distributed and reproduced on top of different hardware, improving reproducibility and dependency management.

**** Docker
Docker is a very popular container system for software development and service deployment. Every major cloud infrastructure provider (such as AWS, Google Cloud Platform, and Microsoft Azure) supports Docker as a platform for executing software, and companies all over the world rely on it to deploy its services.

Docker implements a virtualization model that, by default, isolates as many aspects of the underlying operating system as possible. As a result, a Docker container has many aspects that resemble a traditional virtual machine: it has its own network stack, user space, and file system.

By virtualizing the network stack, Docker relies on a virtual controller that uses Network Address Translation to correlate multiple containers to the host's IP address. This approach forces the user to explicitly specify which ports of the container should be exposed to the host operating system (and the internet), allowing the user to have a finer control over network communication on the container.

Additionally, the user space is also separated between container and host. This means that there is a new root user inside the container, which is controlled by the user who starts it. This makes it easier for the user to customize the container environment, install libraries and packages and make modifications to the virtualized operating system in order to suit the user's needs. On the other hand, it also presents a security concern on shared environments, because the user might be able to exploit the root privileges to gain access on the host machine, as documented in \cite{DockerEscalation}. Docker mitigates this issue by, in its default configuration, allowing only the root user in the host operating system to create containers.

**** Singularity
Singularity is a container system developed for scientific research and high-performance computing applications. Contrary to Docker, Singularity does not aim to create completely isolated environments. It relies on a more conservative virtualization model, with the objective of providing integration with existing tools installed on the host operating system.

Consequently, the only namespace that is isolated between the host and a Singularity container is the file system (meaning that the container can still be a completely different Linux distribution from the host). Every other namespace is not touched by default. Thus, the network stack, process tree, and user space are the same between container and host, which leads to the container being seen as a process which is executed in the host operating system. This feature is very important for two reasons. First, Singularity containers can be started and killed by any tool used to manage processes, such as /mpirun/ or even SLURM. Second, because the user space is not touched, the user that executes processes inside the container is the same as the one which started the container. This means that a regular user can start a container without being granted root access to it. In other words, a given user needs to be root in the host operating system in order to be root inside the container.

*** Experimental Context and Workload Details
**** Experimental environment
The experiments were conducted in the Grid5000 hardware stack. The Grid5000 is a grid platform used for scientific experiments in parallel computing, HPC and computer science. It provides its users with a large number of clusters that can be reserved for exclusive use for a limited time. For this paper, we executed the experiments in the Grid5000's \textit{graphene} cluster, which contains 16GB of DDR3 memory and a quad-core Intel Xeon X3340 on each node. We used up to 64 compute nodes for our tests. Because of the number of cores, each node received a maximum of 4 MPI processes.

The nodes were loaded with a Debian 9 image using the \textit{kadeploy3} tool. To ensure consistency between test cases, the same distribution was used for the virtualized environments in both Docker and Singularity containers. We benchmarked the execution environments with three different applications: NAS EP, Ondes3D and Ping Pong.
**** Benchmarks
The NAS EP is an application included in the NAS Parallel Benchmarks which simulates a parallel random number generator. It is an embarrassingly parallel problem (hence its name), and it was chosen to simulate a highly CPU bound scenario with parallel speedup close to ideal.

Ondes3D is a fluid dynamics simulation application. Its execution signature contains characteristics such as load imbalance and frequent communication between MPI nodes. It was chosen as a mean to add a real-world application signature in this research.

Finally, the Ping Pong benchmark was used to measure the network and communication performance when introducing the container's virtual environment. The experiments, in this case, were conducted between two nodes that exchange MPI messages, with the message size varying from 1 Byte to 1 MByte.

**** Container clusters
The container infrastructure for Docker was built with the cluster proposed by \cite{7868429}. The physical nodes were connected using the Docker Swarm utility, which is responsible for spawning containers on all the nodes and connecting them via an overlay network, so that every container (which will execute an MPI process) can be addressed by the MPI runtime.

The container infrastructure for Singularity is pretty much the same as the one with native processes. Because Singularity containers share the network stack with its host, there is no need for a virtual network between the containers (all the hosts are accessible through the physical network). 

**** Workload details
Two different test suites were run. The first batch covered a smaller problem size of EP and Ondes3D, with 1 to 4 compute nodes. This experiment was executed following a full factorial experimental design with the following factors:
 - Execution environment: Native, Docker and Singularity
 - Parallel compute units (up to 4 per node): 1, 4, 8, 16

The second test suite was aimed at covering a real-world scenario, with a computationally intensive application distributed across many compute nodes. Because of the results obtained in the first test suite, the Docker execution environment was excluded from this simulation (*/I haven't presented the results yet... How should I put this?/*). The experiment was also executed following a full factorial design, with the following factors:
 - Execution environment: Native and Singularity
 - Parallel compute units (up to 4 per node): 64, 128, 192, 256

** TODO Results
The plot (ref) shows the execution time of the NAS EP Benchmark, with respect to the number of parallel executors. Although indicating a slight advantage in the native execution, the plot shows that the virtualized approaches perform very close to the native baseline. This indicates that, when CPU calculation is regarded, none of the container technologies introduce significant overhead. In fact, the difference in execution time can be related to the time needed to spin up the containers. Such an operation does not exist when executing processes in the native operating system.

/*Placeholder for the ep experiment plot*/

Plot (ref) shows the execution time of an Ondes3D simulation of a small scale test, with respect to the number of parallel executors. This plot shows that the performance on the three environments is similar for 1 and 4 executors. However, the Docker performance degrades when considering 8 and 16 processing units. This behavior happens exactly when more physical nodes are added to the experiment, which indicates that the network communication might be impacting the performance of Docker containers. This hypothesis is further supported by the virtual network that is needed to provide connectivity between Docker containers. Such a virtual network does not exist in the other two environments.

/*Placeholder for the Ondes3D Essai plot*/

Plot (ref) presents the Ping Pong benchmark, which was used to measure the communication overhead between nodes. From this experiment, we can see that the network performance on Docker containers is considerably lower when compared to both the native and singularity test cases. This evidence confirms that, as observed in the Ondes3D experiment, the virtual network used by Docker introduces significant overhead to communication. Singularity containers, on the other hand, use the same network stack as the host operating system, resulting in non-observable performance differences.

/*Placeholder for the Ping Pong plot*/

The next plot, (ref), shows a large-scale simulation of the Ligurian earthquake on Ondes3D. This experiment was conducted considering the native execution and Singularity containers, and its main objective is to assess the aggregated overhead of spawning a large number of containers across multiple nodes. As the plot indicates, there is no observable difference in execution time between the two approaches, which indicates that the additional cost of executing applications in a Singularity environment is negligible even when spawning a high number of containers.

/*Placeholder for the Ondes3D Ligurian plot*/

 - How to ignore R source?
 - How to export images into generated tex?
 - Should I add the debian vs alpine experiment? (can support the argument on flexibility, etc)
*** TODO Should we include this argument?
To illustrate the advantages in flexibility for environment configuration, we also conducted an experiment running an Alpine Linux image on the container environments. The Alpine Linux is a lightweight Linux distribution that strives for efficiency and security. It is based on Busybox and provides an alternative set of standard libraries that can yield better performance in some applications. Although installing a completely different Linux distribution on multiple hosts for a single experiment is not feasible (especially in a shared cluster environment), it can be easily done when using containers. The plot (ref) shows how Docker and Singularity (running the Alpine Linux distribution) compare to the native operating system (running Debian). These results show that, by modifying the execution environment, it is possible for the virtualized execution to outperform the native one.

/*Placeholder for the Alpine vs. Debian plot*/

*** EP experiment plot                                           :noexport:
#+begin_src R 
  library(tidyverse)
  
  results <- read_csv('./results/nas/results.csv')
  results <- results %>%
    mutate(time = time/1000) %>%
    group_by(environment, parallelism) %>%
    summarize(
      samples = n(),
      average = mean(time),
      stdDeviation = sd(time),
      stdError = 3*stdDeviation/sqrt(samples)
    )
  results

  custom_theme <- function() {
    ret <- list();
    ret[[length(ret)+1]] <- theme (
      plot.margin = unit(c(0,0,0,0), "cm"),
      legend.spacing = unit(1, "mm"),
      legend.position = "top",
      legend.justification = "left",
      legend.box.spacing = unit(0, "pt"),
      legend.box.margin = margin(0,0,0,0),
      legend.title = element_blank());
    return(ret);
  }

  ggplot(results, aes(x = parallelism, y = average)) +
    scale_x_continuous(breaks = c(1, 4, 8, 16), trans = 'sqrt') +
    ylim(0, NA) +
    geom_point(aes(col = environment), size = 2) +
    geom_line(aes(col = environment), size = 0.5, alpha = 0.3) + 
    geom_errorbar(aes(ymin = average - stdError, ymax = average + stdError, col = environment), width = 0.2) +
    scale_color_grey() + 
    xlab('Amount of computing units (count)') + 
    ylab('Execution time (s)') +
    theme_bw(base_size = 12) +
    theme(legend.position = 'top', legend.spacing = unit(x = c(0, 0, 0, 0), units = 'mm')) +
    custom_theme()
#+end_src

*** Ondes3D ESSAI experiment plot                                :noexport:
#+begin_src R
library(tidyverse);

results <- read_csv('./results/ondes3d/results.csv');

results <- results %>%
  mutate(time = time/1000) %>%
  group_by(environment, parallelism) %>%
  summarize(
    samples = n(),
    average = mean(time),
    stdDeviation = sd(time),
    stdError = 3*stdDeviation/sqrt(samples)
  );

custom_theme <- function() {
  ret <- list();
  ret[[length(ret)+1]] <- theme (
    plot.margin = unit(c(0,0,0,0), "cm"),
    legend.spacing = unit(1, "mm"),
    legend.position = "top",
    legend.justification = "left",
    legend.box.spacing = unit(0, "pt"),
    legend.box.margin = margin(0,0,0,0),
    legend.title = element_blank());
  return(ret);
}

ggplot(results, aes(x = parallelism, y = average)) + 
  geom_line(aes(col=environment), size = 0.5, alpha=0.2) + 
  geom_point(aes(col=environment), size=2) + 
  geom_errorbar(aes(ymin=average-stdError, ymax=average+stdError, col=environment), width=0.15) +
  scale_color_grey() +
  ylim(0, NA) +
  scale_x_continuous(breaks=c(1, 4, 8, 16), trans='sqrt') + 
  xlab("Amount of computing units (count)") +
  ylab("Execution time (s)") +
  theme_bw(base_size=12) +
  theme(legend.position = "top", legend.spacing = unit(x=c(0,0,0,0),units="mm")) +
  custom_theme();
#+end_src

*** Ping Pong plot                                               :noexport:
#+begin_src R
library(tidyverse)

results <- read_csv('./results/ping-pong/results.csv')
results <- results %>% 
  group_by(environment, size) %>%
  summarize(
    samples = n(),
    average = mean(time),
    stdDeviation = sd(time),
    stdError = 3*stdDeviation/sqrt(samples))

default_theme <- function() {
  ret <- list();
  ret[[length(ret)+1]] <- theme (
    plot.margin = unit(c(0,0,0,0), "cm"),
    legend.spacing = unit(1, "mm"),
    legend.position = "top",
    legend.justification = "left",
    legend.box.spacing = unit(0, "pt"),
    legend.box.margin = margin(0,0,0,0),
    legend.title = element_blank());
  return(ret);
}

ggplot(results,aes(x=size, y=average)) +
  geom_line(aes(col = environment), alpha = 0.2) +
  geom_point(aes(col = environment), size = 3) +
  geom_errorbar(aes(ymin=average-stdError, ymax=average+stdError, color=environment, group=environment), width = 0.3) +
  theme_bw(base_size=12) +
  scale_y_continuous(trans='log2') + 
  #ylim(0,NA) +
  scale_x_continuous(trans="log2") + 
  ylab('Average latency (ms)') +
  xlab('Message size (bytes)') +
  scale_color_grey() +
  default_theme()
#+end_src

*** Ondes3D Ligurian plot                                        :noexport:
#+begin_src R
library(tidyverse);

results <- read_csv('./results/ondes3d-ligurian/results.csv');
results <- results %>%
  mutate(time = time/1000) %>%
  group_by(environment, parallelism) %>%
  summarize(
    samples = n(),
    average = mean(time),
    stdDeviation = sd(time),
    stdError = 3*stdDeviation/sqrt(samples)
  );

default_theme <- function() {
  ret <- list();
  ret[[length(ret)+1]] <- theme (
    plot.margin = unit(c(0,0,0,0), "cm"),
    legend.spacing = unit(1, "mm"),
    legend.position = "top",
    legend.justification = "left",
    legend.box.spacing = unit(0, "pt"),
    legend.box.margin = margin(0,0,0,0),
    legend.title = element_blank());
  return(ret);
}

ggplot(results, aes(x = parallelism, y = average)) + 
  geom_line(aes(col=environment), size = 0.5, alpha=0.2) + 
  geom_point(aes(col=environment), size=2) + 
  geom_errorbar(aes(ymin=average-stdError, ymax=average+stdError, col=environment), width=20) +
  scale_color_grey() +
  scale_x_continuous(breaks=seq(64,256,64)) +
  ylim(0, NA) +
  xlab("Amount of computing units (count)") +
  ylab("Execution time (s)") +
  theme_bw(base_size=12) +
  theme(legend.position = "top", legend.spacing = unit(x=c(0,0,0,0),units="mm")) +
  default_theme();
#+end_src

#+RESULTS:

** Conclusion
In this paper, we discussed the use of virtualization technologies in the form of Linux containers to help to solve problems such as reproducibility and user control over HPC environments. Containers provide similar features as hardware level virtualization with a theoretically negligible performance overhead, making them suitable for high-performance applications. In that context, we compared and contrasted two container technologies, Docker and Singularity, against a native environment running with no virtualization.

The results for the proposed tests indicate that containers introduce very little (if any) computational overhead in applications, for both Docker and Singularity. This can be verified by the lack of a clear performance difference on the EP NAS Benchmark, in favor of a specific environment.

Communication overhead, on the other hand, has been observed in Docker containers. This is mainly because the Docker architecture requires the containers to be connected through an overlay network in order for them to have connectivity across multiple hosts (which was needed for the MPI cluster). This overhead was observed in both the Ping Pong test case as well as the Ondes3D application, which is known to require frequent communication between MPI processes. The same communication overhead could not be observed, however, in Singularity containers.

Additionally, we conducted experiments that leveraged the potential flexibility that a virtualized workflow provides. Because containers allow users to fine-tune the execution environment more easily, it was possible to use a different Linux distribution without having root access to the host operating system. This approach yielded better performance than the native execution, which means that it is possible to use these fine-tuning capabilities to considerably enhance the performance of HPC applications.

With the experiments conducted in this research, we can conclude that Linux containers are a suitable option for running HPC applications in a virtualized environment, without the drawbacks of traditional hardware-level virtualization. In our tests, we concluded that Singularity containers are the most suitable option both in terms of system administration (for not granting every user that starts a container root access to the system) and in terms of performance (for not imposing an overlay network that is a potential bottleneck).
** References                                                        :ignore:
# See next section to understand how refs.bib file is created.

#+LATEX: \bibliographystyle{sbc}
#+LATEX: \bibliography{refs}

* Bib file is here                                                 :noexport:

Tangle this file with C-c C-v t

#+begin_src bib :tangle refs.bib

@INPROCEEDINGS{7562612, 
    author={M. T. Chung and N. Quang-Hung and M. T. Nguyen and N. Thoai}, 
    booktitle={2016 IEEE Sixth International Conference on Communications and Electronics (ICCE)}, 
    title={Using Docker in high performance computing applications}, 
    year={2016}, 
    volume={}, 
    number={}, 
    pages={52-57}, 
    keywords={cloud computing;data handling;parallel processing;virtual machines;virtualisation;Docker;HPC;VM;cloud computing;data intensive application;high performance computing;resource management;virtual machines;virtualization technology;Cloud computing;Computer architecture;Containers;Libraries;Virtual machine monitors;Virtual machining;Virtualization;Docker;Graph500;HPC;HPL;cloud computing;performance evaluation}, 
    doi={10.1109/CCE.2016.7562612}, 
    ISSN={}, 
    month={July}
}

@INPROCEEDINGS{7868429, 
    author={N. Nguyen and D. Bein}, 
    booktitle={2017 IEEE 7th Annual Computing and Communication Workshop and Conference (CCWC)}, 
    title={Distributed MPI cluster with Docker Swarm mode}, 
    year={2017}, 
    volume={}, 
    number={}, 
    pages={1-7}, 
    keywords={application program interfaces;containerisation;message passing;parallel processing;source code (software);MPI programs;container orchestration technology;distributed MPI cluster;docker swarm mode;high-performance computing;modern containerization technology;source code;Cloud computing;Computers;Containers;File systems;Linux;Operating systems;Cluster Automation;Container;Distributed System;Docker;Docker Swarm mode;HPC;MPI}, 
    doi={10.1109/CCWC.2017.7868429}, 
    ISSN={}, 
    month={Jan}
}

@article{10.1371/journal.pone.0177459,
    author = {Kurtzer, Gregory M. AND Sochat, Vanessa AND Bauer, Michael W.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Singularity: Scientific containers for mobility of compute},
    year = {2017},
    month = {05},
    volume = {12},
    url = {https://doi.org/10.1371/journal.pone.0177459},
    pages = {1-20},
    abstract = {Here we present Singularity, software developed to bring containers and reproducibility to scientific computing. Using Singularity containers, developers can work in reproducible environments of their choosing and design, and these complete environments can easily be copied and executed on other platforms. Singularity is an open source initiative that harnesses the expertise of system and software engineers and researchers alike, and integrates seamlessly into common workflows for both of these groups. As its primary use case, Singularity brings mobility of computing to both users and HPC centers, providing a secure means to capture and distribute software and compute environments. This ability to create and deploy reproducible environments across these centers, a previously unmet need, makes Singularity a game changing development for computational science.},
    number = {5},
    doi = {10.1371/journal.pone.0177459}
}

@inproceedings{Le:2017:PAA:3093338.3106737,
    author = {Le, Emily and Paz, David},
    title = {Performance Analysis of Applications Using Singularity Container on SDSC Comet},
    booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact},
    series = {PEARC17},
    year = {2017},
    isbn = {978-1-4503-5272-7},
    location = {New Orleans, LA, USA},
    pages = {66:1--66:4},
    articleno = {66},
    numpages = {4},
    url = {http://doi.acm.org/10.1145/3093338.3106737},
    doi = {10.1145/3093338.3106737},
    acmid = {3106737},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {IMB: Intel's MPI Benchmark, NEURON: Neuronal Simulation Tool, OSU: Ohio State University Benchmark, Singularity},
}

@misc{DockerEscalation,
 title = {Docker Privilege Escalation},
 year = {2017},
 url = {https://fosterelli.co/privilege-escalation-via-docker.html}
}

#+end_src
