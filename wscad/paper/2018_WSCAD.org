# -*- coding: utf-8 -*-
# -*- mode: org -*-

#+TITLE: Assessing the Computation and Communication Overhead of Operating System Containers for HPC Applications
#+AUTHOR: Lucas Mello Schnorr

#+STARTUP: overview indent
#+LANGUAGE: pt-br
#+OPTIONS: H:3 creator:nil timestamp:nil skip:nil toc:nil num:t ^:nil ~:~
#+OPTIONS: author:nil title:nil date:nil
#+TAGS: noexport(n) deprecated(d) ignore(i)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [12pt]
#+LATEX_HEADER: \usepackage{sbc-template}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[T1]{fontenc}

# You need at least Org 9 and Emacs 24 to make this work.
# If you do, just type make (thanks Luka Stanisic for this).

* IEEETran configuration for org export + ignore tag (Start Here)  :noexport:

#+begin_src emacs-lisp :results output :session :exports both
(add-to-list 'load-path ".")
(require 'ox-extra)
(ox-extras-activate '(ignore-headlines))
#+end_src

#+RESULTS:

* *The Paper*                                                       :ignore:
** Latex configurations                                             :ignore:

#+BEGIN_EXPORT latex
%\usepackage[brazil]{babel}   
\sloppy
#+END_EXPORT

** Frontpage                                                        :ignore:

#+BEGIN_EXPORT latex
\title{Assessing the Computation and Communication Overhead of Operating System Containers for HPC Applications}

\author{
   Guilherme Rezende Alles\inst{1},
   Alexandre Carissimi\inst{1},
   Lucas Mello Schnorr\inst{1}}

\address{
   Graduate Program in Computer Science (PPGC/UFRGS), Porto Alegre, Brazil}
#+END_EXPORT

#+LaTeX: \maketitle

** Abstract                                                         :ignore:

#+LaTeX: \begin{abstract}

This paper explores the usage of container technologies to solve two relevant problems in high performance computing (HPC) applications. The first problem is reproducibility, expressed by the difficulty of precisely reproducing an execution environment to verify, validate or build on previous work. Because of the amount of potential dependencies and external variables an application depends upon, it can be extremely difficult to simulate the same software environment to reproduce previous research. The second problem is derived from the fact that HPC resources (e.g. clusters and grids) usually have very strict usage policies, that do not allow users to install arbitrary software to run their applications.

#+LaTeX: \end{abstract}

** Introduction

One fundamental problem of scientific computing research is that, in order for it to be credible, it should be reproducible. This means that the researcher has the responsibility of providing its readers with the resources necessary to run experiments and obtain similar results to the ones provided by the research. Although the concept of reproducibility is well known in the scientific community, it is very hard to put it to practice because of the large amount of variables that comprise an execution environment. Additionally, HPC resources are usually managed under strict usage policies, many of which do not allow the user to customize the environment with an arbitrary software stack (e.g. compilers and device drivers), thus making it very difficult to obtain an uniform execution platform across different clusters.

Although presented as difficult to deal with, these problems can be solved through the use of traditional hardware virtualization. By abstracting the hardware layer and providing a general, well known API to the software stack, users can run their own operating system and execution environment on top of a hypervisor, ensuring that the software stack will always be the same regardless of any hardware change. The problem with this approach is that, when considering HPC applications, the performance overhead introduced by the hypervisor is too high for this solution to be considered feasible.

Lately, container technologies have gained a lot of attention in the software development industry. This is mainly because containers are presented as a solution to package enterprise software in a controlled, static environment that will run on a wide variety of Linux systems. When compared to traditional virtual machines, containers have the advantage of not needing a hypervisor for virtualization, yielding performance that is very close to native but still providing application and software stack isolation. 

By using containers to isolate and package HPC applications, we expect to encourage users to use containers to create reproducible, user configurable environments for their applications, thus diminishing the hassle of managing application dependencies and software stack differences when executing experiments across multiple HPC clusters.

In fact, this approach has already been used to a great extent in the software development industry. Container technologies such as Docker are used daily to deploy software microservices to cloud environments, and cloud providers such as AWS and Google Cloud Platform offer services to manage applications based on containers.


Considering the large adoption of containers in software development, we compare and contrast two container technologies, Docker and Singularity, with respect to their performance overhead and ability to solve common HPC workflow problems. For the performance overhead measurements, we will consider the same experiments running on a native environment as the baseline.

** Related Work and Motivation
*** Related Work
As presented in \cite{7562612}, the virtualization of HPC applications is viable in a low overhead environment such as Docker. In this study, the author compares the compute performance of Docker containers to virtual machines, concluding that the former has a considerably lower overhead when compared to the latter. This study, however, only explores the performance of single node applications, and does not present information on how containers can scale to distributed environments.

For multi node computations, \cite{7868429} proposes the creation of an MPI cluster using Docker containers in a distributed environment. In this proposal, the containers are connected through an orchestrator called Docker Swarm, which is responsible for assigning names and providing an overlay network for transparent connectivity between the containers. Performance analysis, however, is absent from this study, obscuring the conclusion of whether such an approach is viable in a real world scenario.

In \cite{10.1371/journal.pone.0177459}, the author introduces Singularity. Singularity is a container system designed for scientific research workflows, and it strives to solve some drawbacks of using Docker containers in HPC. The author discusses how Docker was not designed for shared, multi-user environments (such as supercomputers and HPC centers) and thus presents significant security issues when used in this context. As a consequence, it is very hard to find HPC centers that allow users to use Docker containers. Singularity, on the other hand, was built with these problems in mind and solves them in order to make HPC containers accessible to the scientific community. As a consequence, Singularity containers are already accepted and used in many supercomputers around the world. Additionally, \cite{Le:2017:PAA:3093338.3106737} presents some performance analysis of applications running on top of Singularity containers. It concludes that while some overhead do exist, the reported values are negligible for most use cases. 

*** Motivation
Our objectives for this work is to study the drawbacks and improvements that occur by applying virtualization techniques to high performance computing workflows. As concluded by previous work, using virtual machines is not a feasible approach because of the performance and communication overhead that comes along with this strategy. Thus, our goal is to measure the performance impact of applying virtualization in the form of container technologies to these workloads. We present an analysis covering both synthetic benchmarks and a real application comparing the usability of two major container systems - Docker and Singularity - using a traditional approach (with no virtualization) as a baseline.

Furthermore, we intend to demonstrate that virtualization techniques can be used in HPC without the massive overhead of traditional virtual machines. By using containers, cluster administrators can provide flexibility, portability and enhanced reproducibility to its users without sacrificing performance and security.

** Background and Experimental Context
*** Background
**** Containers
Containers are a mean of achieving virtualization without relying on software to emulate hardware resources. Instead, containers are known as software level virtualization for Linux systems, and they use features that are native to the Linux kernel (namely, \textit{cgroups} and \textit{namespaces}) to isolate the resources managed by the operating system. As a result, software that runs inside of a container can have its own file system, process tree, user space and network stack, giving it the impression of being executed on a completely isolated environment.

By using native kernel features to grant isolation, containers present a theoretically negligible overhead penalty when compared to an application running natively on the host operating system. This happens because the Linux kernel already uses \textit{cgroups} and \textit{namespaces} to manage its resources internally, even when there are not multiple containers on a single machine. Considering this approach, a non-virtualized Linux environment can be seen itself as a single container running on top of the Linux kernel, which means that there is no additional software layer in a container to insert execution overhead.

In spite of being receiving large amounts of attention lately, the core APIs and functionality used to create containers is not new, and have been present in the Linux kernel for more than a decade. However, the popularization of containers took a long time to happen especially because of how difficult it is for an end user to interact with these kernel APIs directly. Conversely, containers only became popular when software (such as Docker and Singularity) was created to interact with the kernel and mediate the creation of containers.

These container management platfors also introduced new features which were very desirable for many workflows (including software development and HPC), such as the ability to encapsulate an entire environment in an image that can be reproduced on top of different hardware, improving reproducibility and dependency management.

**** Docker
Docker is a very popular container system. Every major cloud infrastructure provider (such as AWS, Google Cloud Platform and Microsoft Azure) supports Docker as a platform for executing software, and companies all over the world rely on it to deploy services.

Docker implements a virtualizarion model that, by default, isolates as many aspects of the underlying operating system as possible. As a result, a Docker container has many aspects that resemble a traditional virtual machine: it has its own network stack, user space and file system.

By virtualizing the network stack, Docker relies on a virtual controller that uses Network Address Translation to correlate multiple containers to the host's IP address. This approach forces the user do explicitly specify which ports of the container should be exposed to the host operating system (and the internet), allowing the user to have a finer control over network communication on the container.

Additionally, the user space is also separated between container and host. This means that there is a new root user inside the container, which is controlled by the user who starts said container. This makes it easier for the user to customize the container environment, install libraries and packages and make modifications on the virtualized operating system to suit the user's needs. On the other hand, it also presents a security concern on shared environments, because the user might be able to exploit the root privileges to gain access on the host machine, as documented in \cite{DockerEscalation}. Docker mitigates this issue by, in its default configuration, allowing only the root user in the host operating system to create containers.

**** Singularity
Singularity is a container system developed for scientific research and high performance computing applications. Contrary to Docker, Singularity does not aim to create completely isolated environments. It relies on a more conservative virtualization model, with the objective of providing integration with existing tools installed on the host operating system.

Consequently, the only namespace that is isolated between the host and a Singularity container is the file system (meaning that the container can still be a completely different Linux distribution from the host). Every other namespace is not touched by default. Thus, the network stack, process tree and user space are the same between container and host, which leads to the container being seen as a process which is executed in the host operating system. This feature is very important for two reasons. First, Singularity containers can be started and killed by any tool used to manage processes, such as /mpirun/ or even SLURM. Second, because the user space is not touched, the user that executes processes inside the container is the same as the one which started the container. This means that a regular user can start a container without being granted root access to it. In other words, a given user needs to be root in the host operating system in order to be root inside the container.

*** Experimental Context and Workload Details

**** Experimental environment
The experiments were conducted in the Grid5000 hardware stack. The Grid5000 is a grid platform used for scientific experiments in parallel computing, HPC and computer science. It provides its users with a large amount of clusters that can be reserved for exclusive use for a limited time. For this paper, we executed the experiments in the Grid5000's \textit{graphene} cluster, which contains 16GB of DDR3 memory and a quad core Intel Xeon X3340 on each node. We used up to 64 compute nodes for our tests. Because of the number of cores, each node received a maximum of 4 MPI processes.

The nodes were loaded with a Debian 9 image using the \textit{kadeploy3} tool. To ensure consistency between test cases, the same distribution was used for the virtualized environments in both Docker and Singularity containers. We benchmarked the execution environments with three different applications: NAS EP, Ondes3D and Ping Pong.

**** Benchmarks
The NAS EP is an application included in the NAS Parallel Benchmarks which simulates a parallel random number generator. It is an embarassingly parallel problem (hence its name), and it was chosen to simulate a highly CPU bound scenario with parallel speedup close to ideal.

Ondes3D is a fluid dynamics simulation application. Its execution signature contains characteristics such as load imbalance and frequent communication between MPI nodes. It was chosen as a mean to include a real world scenario in this research.

Finally, the Ping Pong benchmark was used to measure the network and communication performance when introducing the container's virtual environment. The experiments in this case were conducted between two nodes that exchange MPI messages between each other. The message size was varied from 1 Byte to 1 MByte.

**** Container clusters
The container infrastructure for Docker was built with the cluster proposed by \cite{7868429}. The physical nodes were connected using the Docker Swarm utility, which is responsible for spawning containers on all the nodes and connecting them via an overlay network.

The container infrastructure for Singularity is pretty much the same as the one with native processes. The only difference is that instead of distributing the application binary, I distributed the container image.

**** Workload details
Two different test suites were run. The first batch covered a smaller problem size of EP and Ondes3D, with 1 to 4 compute nodes. This experiment was executed following a full factorial experimental design with the following factors:
 - Execution environment: Native, Docker and Singularity
 - Parallel compute units (up to 4 per node): 1, 4, 8, 16

The second test suite was aimed at covering a real world scenario, with a computationally intensive application distributed across many compute nodes. Because of the results obained in the first test suite, the Docker execution environment was excluded from this simulation (*/I haven't presented the results yet... How should I put this?/*). The experiment was also executed following a full factorial design, with the following factors:
 - Execution environment: Native and Singularity
 - Parallel compute units: 64, 128, 192, 256

** TODO Results
 - How to ignore R source?
 - How to export images into generated tex?
#+begin_src R
  library(tidyverse)
  
  results <- read_csv('./results/nas/results.csv')
  results <- results %>%
    mutate(time = time/1000) %>%
    group_by(environment, parallelism) %>%
    summarize(
      samples = n(),
      average = mean(time),
      stdDeviation = sd(time),
      stdError = 3*stdDeviation/sqrt(samples)
    )
  results

  custom_theme <- function() {
    ret <- list();
    ret[[length(ret)+1]] <- theme (
      plot.margin = unit(c(0,0,0,0), "cm"),
      legend.spacing = unit(1, "mm"),
      legend.position = "top",
      legend.justification = "left",
      legend.box.spacing = unit(0, "pt"),
      legend.box.margin = margin(0,0,0,0),
      legend.title = element_blank());
    return(ret);
  }

  ggplot(results, aes(x = parallelism, y = average)) +
    scale_x_continuous(breaks = c(1, 4, 8, 16), trans = 'sqrt') +
    ylim(0, NA) +
    geom_point(aes(col = environment), size = 2) +
    geom_line(aes(col = environment), size = 0.5, alpha = 0.2) + 
    geom_errorbar(aes(ymin = average - stdError, ymax = average + stdError, col = environment), width = 0.2) +
    scale_color_grey() + 
    xlab('Amount of computing units (count)') + 
    ylab('Execution time (s)') +
    theme_bw(base_size = 12) +
    theme(legend.position = 'top', legend.spacing = unit(x = c(0, 0, 0, 0), units = 'mm')) +
    custom_theme()
#+end_src

#+RESULTS:

Show experiment plots (approx 3 to 4 plots)

Discuss come conclusions derived from the results
** Conclusion
In this paper we discussed the problem of reproducibility and user control over HPC environments, and how it can be solved with the usage of container technologies. Containers provide similar features as hardware level virtualization, but with a theoretically negligible performance overhead, making them suitable for high performance applications. In that context, we compared and contrasted two container technologies, Docker and Singularity, against a native environment running with no virtualization.

The results for the proposed tests indicate that containers introduce very little (if any) computational overhead in applications, for both Docker and Singularity. This can be verified by the lack of a clear performance difference on the EP NAS Benchmark, in favor of a specific environment.

Communication overhead, on the other hand, has been observed in Docker containers. This is mainly because Docker requires the containers to be connected through an overlay network in order for them to have connectivity across multiple hosts (which was needed for the MPI cluster). This overhead was observed in both the Ping Pong test case as well as the Ondes3D application, which is known to require frequent communication between MPI processes. The same communication overhead could not be ovserved, however, in Singularity containers.

With the experiments conducted in this research, we can conclude that Linux containers are a suitable option for running HPC applications in a virtualized environment, without the drawbacks of traditional hardware level virtualization. In our tests, we concluded that Singularity containers are the most suitable option both in terms of system administration (for not granting every user that starts a container root access to the system) and in terms of performance (for not imposing an overlay network that is a potential bottleneck).

** Acknowledgments                                                  :ignore:
#+LATEX:\section*{Acknowledgements}

Who paid for this?

** References                                                        :ignore:
# See next section to understand how refs.bib file is created.

#+LATEX: \bibliographystyle{sbc}
#+LATEX: \bibliography{refs}

* Bib file is here                                                 :noexport:

Tangle this file with C-c C-v t

#+begin_src bib :tangle refs.bib

@INPROCEEDINGS{7562612, 
    author={M. T. Chung and N. Quang-Hung and M. T. Nguyen and N. Thoai}, 
    booktitle={2016 IEEE Sixth International Conference on Communications and Electronics (ICCE)}, 
    title={Using Docker in high performance computing applications}, 
    year={2016}, 
    volume={}, 
    number={}, 
    pages={52-57}, 
    keywords={cloud computing;data handling;parallel processing;virtual machines;virtualisation;Docker;HPC;VM;cloud computing;data intensive application;high performance computing;resource management;virtual machines;virtualization technology;Cloud computing;Computer architecture;Containers;Libraries;Virtual machine monitors;Virtual machining;Virtualization;Docker;Graph500;HPC;HPL;cloud computing;performance evaluation}, 
    doi={10.1109/CCE.2016.7562612}, 
    ISSN={}, 
    month={July}
}

@INPROCEEDINGS{7868429, 
    author={N. Nguyen and D. Bein}, 
    booktitle={2017 IEEE 7th Annual Computing and Communication Workshop and Conference (CCWC)}, 
    title={Distributed MPI cluster with Docker Swarm mode}, 
    year={2017}, 
    volume={}, 
    number={}, 
    pages={1-7}, 
    keywords={application program interfaces;containerisation;message passing;parallel processing;source code (software);MPI programs;container orchestration technology;distributed MPI cluster;docker swarm mode;high-performance computing;modern containerization technology;source code;Cloud computing;Computers;Containers;File systems;Linux;Operating systems;Cluster Automation;Container;Distributed System;Docker;Docker Swarm mode;HPC;MPI}, 
    doi={10.1109/CCWC.2017.7868429}, 
    ISSN={}, 
    month={Jan}
}

@article{10.1371/journal.pone.0177459,
    author = {Kurtzer, Gregory M. AND Sochat, Vanessa AND Bauer, Michael W.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Singularity: Scientific containers for mobility of compute},
    year = {2017},
    month = {05},
    volume = {12},
    url = {https://doi.org/10.1371/journal.pone.0177459},
    pages = {1-20},
    abstract = {Here we present Singularity, software developed to bring containers and reproducibility to scientific computing. Using Singularity containers, developers can work in reproducible environments of their choosing and design, and these complete environments can easily be copied and executed on other platforms. Singularity is an open source initiative that harnesses the expertise of system and software engineers and researchers alike, and integrates seamlessly into common workflows for both of these groups. As its primary use case, Singularity brings mobility of computing to both users and HPC centers, providing a secure means to capture and distribute software and compute environments. This ability to create and deploy reproducible environments across these centers, a previously unmet need, makes Singularity a game changing development for computational science.},
    number = {5},
    doi = {10.1371/journal.pone.0177459}
}

@inproceedings{Le:2017:PAA:3093338.3106737,
    author = {Le, Emily and Paz, David},
    title = {Performance Analysis of Applications Using Singularity Container on SDSC Comet},
    booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact},
    series = {PEARC17},
    year = {2017},
    isbn = {978-1-4503-5272-7},
    location = {New Orleans, LA, USA},
    pages = {66:1--66:4},
    articleno = {66},
    numpages = {4},
    url = {http://doi.acm.org/10.1145/3093338.3106737},
    doi = {10.1145/3093338.3106737},
    acmid = {3106737},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {IMB: Intel's MPI Benchmark, NEURON: Neuronal Simulation Tool, OSU: Ohio State University Benchmark, Singularity},
}

@misc{DockerEscalation,
 title = {Docker Privilege Escalation},
 year = {2017},
 url = {https://fosterelli.co/privilege-escalation-via-docker.html}
}

#+end_src
