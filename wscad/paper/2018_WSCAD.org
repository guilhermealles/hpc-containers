
# -*- coding: utf-8 -*-
# -*- mode: org -*-

#+TITLE:
#+AUTHOR: Lucas Mello Schnorr

#+STARTUP: overview indent
#+LANGUAGE: en-us
#+OPTIONS: H:3 creator:nil timestamp:nil skip:nil toc:nil num:t ^:nil ~:~
#+OPTIONS: author:nil title:nil date:nil
#+TAGS: noexport(n) deprecated(d) ignore(i)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [12pt]
#+LATEX_HEADER: \usepackage{sbc-template}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{hyperref}

# You need at least Org 9 and Emacs 24 to make this work.
# If you do, just type make (thanks Luka Stanisic for this).

* WSCAD 2018 Submission Rules                                      :noexport:

WSCAD Simpósio em Sistemas Computacionais de Alto Desempenho

O Simpósio em Sistemas Computacionais de Alto Desempenho (WSCAD) é um
evento anual que apresenta as principais aplicações, desenvolvimentos
e tendências nas áreas de arquitetura de computadores, processamento
de alto desempenho e sistemas distribuídos. Na sua décima nona edição
o WSCAD será realizado na cidade de São Paulo.

Em paralelo ao WSCAD ocorre, o Concurso de Teses e Dissertações em
Arquitetura de Computadores e Computação de Alto Desempenho
(WSCAD-CTD), o Workshop de Iniciação Científica em Arquitetura de
Computadores e Computação de Alto Desempenho (WSCAD-WIC), o Workshop
sobre Educação em Arquitetura de Computadores (WEAC), o Workshop em
Computação Heterogênea (WCH) e a Maratona Internacional de Programação
Paralela, entre outros eventos a serem anunciados.

Os(as) autores(as) interessados(as) em apresentar seus trabalhos na
trilha principal do WSCAD deverão submeter seus artigos em português
ou inglês em formato PDF através da plataforma JEMS.

Os artigos serão avaliados pelos membros do comitê de programa e por
revisores externos ao comitê. Os anais serão publicados na BDBComp da
SBC e os melhores trabalhos serão convidados a submeter para uma
edição especial de um periódico internacional.

A submissão de artigos para a trilha principal do WSCAD em breve
estará aberta. Os artigos submetidos poderão ser escritos em lingua
portuguesa ou inglesa e deverão ter um limite de 12 páginas (incluindo
figuras, tabelas e referências) seguindo o formato da SBC para
submissão de artigos.  Datas importantes:

    Submissão de trabalhos: 13/07/2018
    Notificação de aceitação: 20/08/2018
    Envio da versão final: 30/08/2018

* IEEETran configuration for org export + ignore tag (Start Here)  :noexport:

#+begin_src emacs-lisp :results output :session :exports both
(add-to-list 'load-path ".")
(require 'ox-extra)
(ox-extras-activate '(ignore-headlines))
#+end_src

#+RESULTS:

* *The Paper*                                                          :ignore:
** Latex configurations                                             :ignore:

#+BEGIN_EXPORT latex
%\usepackage[brazil]{babel}   
\def\COMPANION{{\scriptsize\url{https://github.com/guilhermealles/hpc-containers/}}}
#+END_EXPORT

** Frontpage                                                        :ignore:

#+BEGIN_EXPORT latex
\title{Assessing the Computation and Communication \\ Overhead of Linux Containers for HPC Applications}

\author{
   Guilherme Rezende Alles,
   Alexandre Carissimi,
   Lucas Mello Schnorr}

\address{Instituto de Informática -- Universidade Federal do Rio Grande do Sul (UFRGS)\\
  Caixa Postal 15.064 -- 91.501-970 -- Porto Alegre -- RS -- Brazil
  \email{\{gralles,asc,schnorr\}@inf.ufrgs.br}
  }
#+END_EXPORT

#+LaTeX: \maketitle

** Abstract                                                         :ignore:

#+LATEX: \begin{abstract}
Virtualization technology provides features that are desirable for
high-performance computing (HPC), such as enhanced reproducibility for
scientific experiments and flexible execution environment
customization. The paper explores the performance implications of
applying Operating System (OS) containers in HPC applications. Docker and
Singularity are compared to a native baseline with no virtualization,
using synthetic workloads and an earthquake simulator called Ondes3D
as benchmarks. Our evaluation using up to 256 cores indicate that (a)
Singularity containers have minor performance overhead, (b) Docker
containers do suffer from increased network latency, and (c)
performance gains are attainable with an optimized container on top of
a regular OS.
#+LATEX: \end{abstract}

#+BEGIN_COMMENT LUCAS
I think it would be nice to give some context information in the
beginning of the abstract. First phrase is also disconnected with
the "assessing the computation and comm. overhead ...". Additionally,
you can include some details about the methodology and the main
results. It is okay to reveal this from the start.
#+END_COMMENT

** TODO Introduction

One fundamental aspect of scientific computing research is that, in
order for it to be credible, it should be reproducible. This means
that the researcher has the responsibility of providing its readers
with the resources necessary to run experiments and obtain similar
results to the ones provided by the research. Although the concept of
reproducibility is well known in the scientific community, it is very
hard to put it to practice because of the large number of variables
that comprise an execution environment. Additionally, HPC resources
are usually managed under strict usage policies, many of which do not
allow the user to customize the environment with an arbitrary software
stack (e.g. compilers, libraries or device drivers), thus making it
very difficult to obtain a uniform execution platform for a given 
experiment across different clusters.

#+BEGIN_COMMENT LUCAS
I don't understand why a "uniform execution platform across different
clusters" would be necessary in HPC. Do you mean having a single
software stack to manage ALL experiments in HPC?
#+END_COMMENT
#+BEGIN_COMMENT ALLES
No, I mean having a single software stack for an experiment 
regardless of the hardware (cluster/supercomputer/whatever) that 
executes it. Say you run your experiments on a  Debian cluster and 
all I have available is a CentOS cluster. The uniform execution platform 
would be relevant because it would not matter which distribution is 
installed natively.
#+END_COMMENT

Although being intuitively difficult to deal with, these problems can
be solved through the use of traditional hardware virtualization, in 
which users can run their own operating system and execution 
environment on top of a hypervisor, ensuring that the software stack 
can be configured to the exact requirements for a given experiment. 
The problem with this approach is that, when considering HPC 
applications, the performance overhead introduced by the hypervisor 
and an entire virtual machine is too high for this solution to be 
considered feasible.

#+BEGIN_COMMENT LUCAS
It is interesting to include some citations that help the flow.  There
are no citations so far and until the end of the introduction. All
motivation and problem definition need to be based on context that can
be cited.
#+END_COMMENT
#+BEGIN_COMMENT ALLES
Okay, agreed. I will look into that.
#+END_COMMENT

As an alternative to virtual machines, container technologies have 
gained a lot of attention in the software development industry. This 
is mainly because containers offer the benefits of virtualization 
at a fraction of the performance overhead introduced by virtual 
machines \cite{7562612}.

By using containers to isolate and package software, users are able to
create reproducible, configurable environments for their applications,
thus diminishing the hassle of managing application dependencies and
software stack differences when executing experiments across different
Linux environments.

Considering the large adoption of containers in software development,
this paper compares and contrast two container technologies, Docker and
Singularity, with respect to their performance overhead and ability to
solve common HPC workflow problems. For the performance measurements,
we will consider the same experiments running on a native environment
as the baseline.

#+BEGIN_COMMENT Lucas
This introduction lacks to clearly define the problem. Why such
comparison is necessary? Why it is important to evaluate performance?
Is there a performance problem? Do you want to make sure others can
re-use containers without suffering from performance penalties?

At some point (perhaps rephrasing the last paragraph -- see above),
you should start a phrase with "This paper ..." to explicitly tell the
reader what this paper is about. We can also itemize the three
contributions and a similar way done in the abstract, but with further
details.

Most of the introduction introduces historical and basic concepts
about virtualization techniques, and IMHO fail to bring the reader
quickly to the problem addressed in the paper and its
contribution. More emphasis is given to such basic concepts to what
this paper is about (context, problem, solution, contributions). Some
text here could be reused in the Section [[#sec.background]].

Write this at the very end of the introduction (end of paper structure
paragraph): "The companion material of this work, including the source
code, analysis scripts, and raw measurements, is publicly available
at \COMPANION."
#+END_COMMENT

Section [[#sec.background]] presents some basic concepts regarding 
container technologies, Docker, and Singularity. Section
[[#sec.relatedwork]] discusses related work in the field of containers 
and virtualization in high-performance computing contexts. 
# Section 
# [[#sec.experiments]] presents the experimental design and benchmarks that 
# were used for the tests conducted in this paper.
Section [[#sec.results]] 
presents the results and brief conclusions obtained from the 
experiments. The companion material of this work, including the source 
code, analysis scripts, and raw measurements, is publicly available 
at \COMPANION.

** Basic Concepts
:PROPERTIES:
:CUSTOM_ID: sec.background
:END:

#+BEGIN_COMMENT LUCAS
This empty space is considered to be a bad style choice.
#+END_COMMENT

#+BEGIN_COMMENT LUCAS
This empty space is considered to be a bad style choice.
#+END_COMMENT

#+BEGIN_COMMENT LUCAS
This section should be just after the introduction. It would help to
understand the related work.
#+END_COMMENT

*** Containers

Containers are a mean of achieving virtualization without relying on
software to emulate hardware resources. Instead, containers are known
as software level virtualization for Linux systems, and they use
features that are native to the Linux kernel (namely, \textit{cgroups}
and \textit{namespaces}) to isolate the resources managed by the
operating system. As a result, software that runs inside of a
container can have its own file system, process tree, user space and
network stack, giving it the impression of being executed on a
completely isolated environment.

By using native kernel features to grant isolation, containers present
a theoretically negligible overhead penalty when compared to an
application running natively on the host operating system. This
happens because the Linux kernel already uses \textit{cgroups} and
\textit{namespaces} to manage its resources internally, even when
there are not multiple containers on a single machine. Considering
this approach, a non-virtualized Linux environment can be seen itself
as a single container running on top of the Linux kernel, which means
that there is no additional software layer in a container that should
insert execution overhead.

In spite of being receiving large amounts of attention lately, the
core APIs and functionality used to create containers is not new, and
have been present in the Linux kernel for more than a decade. However,
the popularization of containers took a long time to happen especially
because of how difficult it is for an end user to interact with these
kernel APIs directly. Conversely, containers only became popular when
software (such as Docker and Singularity) was created to interact with
the kernel and mediate the creation of containers.

These container management platforms also introduced new features
which were very desirable for many workflows (including software
development and HPC), such as the ability to encapsulate an entire
environment in an image that can be distributed and reproduced on top
of different hardware, improving reproducibility and dependency
management.

#+BEGIN_COMMENT LUCAS
Are there only two container technologies (Docker and Singularity)?
Cite all of them and describe what they have in common. Justify why
only two of them are adopted in this work. While you could argue that
they have more acceptance by the community, a technical argument would
be better.
#+END_COMMENT

*** Docker

Docker is a very popular container system for software development and
service deployment. Every major cloud infrastructure provider (such as
AWS, Google Cloud Platform, and Microsoft Azure) supports Docker as a
platform for executing software, and companies all over the world rely
on it to deploy its services.

Docker implements a virtualization model that, by default, isolates as
many aspects of the underlying operating system as possible. As a
result, a Docker container has many aspects that resemble a
traditional virtual machine: it has its own network stack, user space,
and file system.

By virtualizing the network stack, Docker relies on a virtual
controller that uses Network Address Translation to correlate multiple
containers to the host's IP address. This approach forces the user to
explicitly specify which ports of the container should be exposed to
the host operating system (and the internet), allowing the user to
have a finer control over network communication on the container.

Additionally, the user space is also separated between container and
host. This means that there is a new root user inside the container,
which is controlled by the user who starts it. This makes it easier
for the user to customize the container environment, install libraries
and packages and make modifications to the virtualized operating
system in order to suit the user's needs. On the other hand, it also
presents a security concern on shared environments, because it is
possible for the user to mount the root directory from the host
operating system as a volume in the container, thus granting access to
all the files in the host machine. Docker mitigates this issue by, in
its default configuration, allowing only the root user in the host
operating system to create containers.

*** Singularity

Singularity is a container system developed for scientific research
and high-performance computing applications. Contrary to Docker,
Singularity does not aim to create completely isolated
environments. It relies on a more conservative virtualization model,
with the objective of providing integration with existing tools
installed on the host operating system.

Consequently, the only namespace that is isolated between the host and
a Singularity container is the file system (meaning that the container
can still be a completely different Linux distribution from the
host). Every other namespace is not touched by default. Thus, the
network stack, process tree, and user space are the same between
container and host, which leads to the container being seen as a
process which is executed in the host operating system. This feature
is very important for two reasons. First, Singularity containers can
be started and killed by any tool used to manage processes, such as
/mpirun/ or even SLURM. Second, because the user space is not touched,
the user that executes processes inside the container is the same as
the one which started the container. This means that a regular user
can start a container without being granted root access to it. In
other words, a given user needs to be root in the host operating
system in order to be root inside the container.
** Related Work and Motivation
:PROPERTIES:
:CUSTOM_ID: sec.relatedwork
:END:

#+BEGIN_COMMENT LUCAS
This empty space is considered to be a bad style choice.
#+END_COMMENT

*** Related Work

#+BEGIN_COMMENT LUCAS
Instead of jumping in directly to the citations; give some general
context information about related work. Explicitely tell the reader
that you will list other works that /evaluate performance/ in container
environments in the HPC context. A very brief historical perspective
is also welcome, sometimes.
#+END_COMMENT

The virtualization of HPC applications is viable in a low overhead
environment such as Docker \cite{7562612}. In this study,
the author compares the compute performance of Docker containers to
virtual machines, concluding that the former has a considerably lower
overhead when compared to the latter. This study, however, only
explores the performance of single node applications, and does not
present information on how containers can scale to distributed
environments which are often the case for high-performance
applications.

For multi-node computations, another investigation \cite{7868429}
work proposes the creation of
an MPI cluster using Docker containers in a distributed
environment. In this proposal, the containers are connected through an
orchestrator called Docker Swarm, which is responsible for assigning
names and providing an overlay network for transparent connectivity
between the containers. Performance analysis, however, is absent from
this study, obscuring the conclusion of whether such an approach is
viable in a real-world scenario.

#+BEGIN_COMMENT LUCAS
The first part of this paragraph (around
\cite{10.1371/journal.pone.0177459}) looks like basic concepts and
historical perspective to be included in Section [[#sec.background]]. Only
at the end you talk about another work that carried out a performance
analysis of Singularity; but no details are given about platform,
workload.
#+END_COMMENT

Singularity \cite{10.1371/journal.pone.0177459} is a container system designed for scientific
research workflows, and it strives to solve some drawbacks of using
Docker containers in HPC. The author discusses how Docker is not
designed for shared, multi-user environments (such as supercomputers
and HPC centers) and thus presents significant security issues when
used in this context. As a consequence, it is very hard to find HPC
centers that allow users to execute Docker containers. Singularity, on
the other hand, is built with these problems in mind and solves them
in order to make HPC containers accessible to the scientific
community. Consequently, Singularity containers are already accepted
and used in many supercomputers around the world. Additionally,
a performance analysis of applications running on top of Singularity
containers has also been carried out
\cite{Le:2017:PAA:3093338.3106737}. It concludes that while some
overhead does exist, the reported values are negligible for most use
cases.

*** Motivation

Wrap up the state of the art mentioned in previous work, mention what is missing, present objectives and motivation.

#+BEGIN_COMMENT LUCAS
Perhaps a table like this could be useful. Other criteria could be added.

| Related Work                       | Container         | Nodes     | Workload | Conclusions                 |
|------------------------------------+-------------------+-----------+----------+-----------------------------|
| \cite{7562612}                     | Docker            | Single    | ?        | Docker more viable than VM  |
| \cite{7868429}                     | Docker with swarm | How many? | ?        | Perf. Analysis inconclusive |
| \cite{Le:2017:PAA:3093338.3106737} | Singularity       | ?         | ?        | ?                           |
| ?                                  |                   |           |          |                             |
| ?                                  |                   |           |          |                             |

Notice the two empty rows to tell you that more is necessary.
#+END_COMMENT


The goal of this work is to study the drawbacks and
improvements that occur by applying virtualization techniques to
high-performance computing workflows. As concluded by previous work,
using virtual machines is unfeasible approach because of the
performance and communication overhead that comes along with this
strategy. Thus, our goal is to measure the performance impact of
applying virtualization in the form of container technologies to these
workloads. We present an analysis covering both synthetic benchmarks
and a real application comparing the usability of two major container
systems - Docker and Singularity - and using a traditional approach
(with no virtualization) as a baseline.

Furthermore, we intend to demonstrate that virtualization techniques
can be used in HPC without the massive overhead of traditional virtual
machines. By using containers, cluster administrators can provide
flexibility, portability and enhanced reproducibility to its users
without sacrificing performance and security.
** Results and Evaluation of the Performance Overhead
:PROPERTIES:
:CUSTOM_ID: sec.results
:END:

Results are based on measurements obtained from experiments with
multiple compute nodes of the Grid5000 platform \cite{grid5000}, in a
controlled HW/SW environment. In what follows, we present (a) the
software/hardware configuration adopted across all experiments with
three cases (Native, Docker, Singularity) and three benchmarks
(NAS-EP, Ondes3D, Ping-Pong); (b) the computation overhead analysis
with a comparison between docker, singularity, and native; (c) a
verification of the increased communication latency leading to bad
application performance; and (d) a comprehensive analysis to verify
how performance gains can be used solely in applying an optimized
container on top of an optimized OS.

*** SW/HW Environment, Benchmarks, and Workload Details
**** Introduction                                                 :ignore:

The Grid5000 is a platform used for scientific experiments in parallel
computing, HPC, and computer science. It provides its users with many
clusters that can be reserved for exclusive use for a limited time. We
executed the experiments in the Grid5000's =graphene= cluster (at
Nancy), which contains 131 nodes, each one equipped with 16GB of DDR3
memory and a quad-core Intel Xeon X3340 (Lynnfield, 2.53GHz), and
interconnected by a 1Gigabit Ethernet and a 20Gbps Infiniband
network. We used up to 64 compute nodes for our tests using
exclusively the 1Gigabit Ethernet because of limitations in the
container configuration. In all experiments, each node received a
maximum of 4 MPI processes due to the 4-core availability of processor
cores. All compute nodes are initially deployed (using =kadeploy3=
\cite{jeanvoine2013kadeploy3}) with the default Debian9 OS image,
before laying the Docker or Singularity environment on top of it.

Three execution environments are configured: Native, Docker, and
Singularity.  To ensure consistency between the container environments
against the Native case, the same Debian9 Linux distribution was used for
such environments in both Docker and Singularity containers.  We have
used a previously proposed \cite{7868429} multi-node container
infrastructure for Docker where physical nodes are connected using the
Docker Swarm utility. This tool is responsible for spawning containers
on all the nodes and connecting them via an overlay network, so that
every container (which will execute an MPI process) can be addressed
by the MPI middleware. The multi-node container infrastructure for
Singularity is similar to the one with native processes. Because
Singularity containers share the network stack with its host, there is
no need for a virtual network between the containers. Therefore, all
the hosts are accessible through the physical network.

Three parallel applications are used to evaluate the performance in
the OS options (Native, Docker and Singularity): NAS-EP, Ondes3D, and
Ping-Pong, detailed as follows.

**** Benchmarks and Workload Details
:PROPERTIES:
:UNNUMBERED: t
:END:

The NAS Embarrassingly Parallel -- *NAS-EP* -- is part of the NAS
Parallel Benchmarks (NPB) \cite{bailey1991parallel}. NAS-EP generates
independent Gaussian random numbers using the polar method, being
considered a CPU-bound case with parallel speedup close to ideal since
communication takes place in the beginning and end of the
execution. EP is executed with the class B workload using one to four
hosts (4 to 16 cores) in preliminary tests. *Ondes3D* \cite{dupros:10}
is developed at the BRGM (French Geological Survey) as an
implementation of the finite-differences method (FDM) to simulate the
propagation of seismic waves in three-dimensional media. As previously
observed \cite{tesser2017using}, its signature contains
characteristics such as load imbalance and frequent asynchronous
small-message communications among MPI ranks. So, this real-world
application is also evaluated to verify if it is impacted by OS
containers. Finally, an in-house *Ping-Pong* benchmark developed with
MPI (see the companion material for the source code) was used to
assess the bandwidth and latency performance when introducing the
container's virtual environment.

The experiments, in this case, were conducted between two nodes that
exchange MPI messages, with the message size varying from 1 Byte to 1
MByte.

We benchmarked the execution environments with three different
applications: NAS EP, Ondes3D and Ping Pong.

#+BEGIN_COMMENT LUCAS
Cite Jain 1991 when talking about the "full factorial design". A table
here could help to create a summary. Further details about workload
are necessary.
#+END_COMMENT

Two different test suites were run. The first batch covered a smaller
problem size of EP and Ondes3D, with 1 to 4 compute nodes. This
experiment was executed following a full factorial experimental design
with the following factors:
 - Execution environment: Native, Docker and Singularity
 - Parallel compute units (up to 4 per node): 1, 4, 8, 16

The second test suite was aimed at covering a real-world scenario,
with a computationally intensive application distributed across many
compute nodes. We used Ondes3D as a platform to simulate the
propagation of the Ligurian earthquake, which happened in 1887. The
experiment was also executed following a full factorial design, with
the following factors:
 - Execution environment: Native, Docker and Singularity
 - Parallel compute units (up to 4 per node): 64, 128, 192, 256

*** Computation Overhead Analysis

*** Verification of Increased Communication Latency

*** Performance Gains due to Optimized Container

*** Previous text                                                :noexport:

All the experiments were executed with multiple replications. In the
following plots, the reported values are the average obtained across
10 executions of each experiment, and the error bars indicate the
confidence interval for 99% confidence.

The plot (ref) shows the execution time of the NAS EP Benchmark, with
respect to the number of parallel executors. Although indicating a
slight advantage in the native execution, the plot shows that the
virtualized approaches perform very close to the native baseline. This
indicates that, when CPU calculation is regarded, none of the
container technologies introduce significant overhead. In fact, the
difference in execution time can be related to the time needed to spin
up the containers. Such an operation does not exist when executing
processes in the native operating system.

#+LATEX: \begin{figure}[h]
#+LATEX: \centering
#+LATEX: \includegraphics[width=.8\textwidth]{./img/ep-b.png}
#+LATEX: \caption{Execution time for the NAS EP benchmark}
#+LATEX: \end{figure}

Plot (ref) shows the execution time of an Ondes3D simulation of a
small scale test, with respect to the number of parallel
executors. This plot shows that the performance on the three
environments is similar for 1 and 4 executors. However, the Docker
performance degrades when considering 8 and 16 processing units. This
behavior happens exactly when more physical nodes are added to the
experiment, which indicates that the network communication might be
impacting the performance of Docker containers. This hypothesis is
further supported by the virtual network that is needed to provide
connectivity between Docker containers. Such a virtual network does
not exist in the other two environments.

#+LATEX: \begin{figure}[h]
#+LATEX: \centering
#+LATEX: \includegraphics[width=.8\textwidth]{./img/ondes3d-essai-50ts.png}
#+LATEX: \caption{Execution time for the Ondes3D ESSAI simulation}
#+LATEX: \end{figure}

Plot (ref) presents the Ping Pong benchmark, which was used to measure
the communication overhead between nodes. From this experiment, we can
see that the network performance on Docker containers is considerably
lower when compared to both the native and singularity test
cases. This evidence confirms that, as observed in the Ondes3D
experiment, the virtual network used by Docker introduces significant
overhead to communication. Singularity containers, on the other hand,
use the same network stack as the host operating system, resulting in
non-observable performance differences.

#+LATEX: \begin{figure}[h]
#+LATEX: \centering
#+LATEX: \includegraphics[width=.8\textwidth]{./img/ping-pong.png}
#+LATEX: \caption{Average network latency measured with the Ping Pong benchmark}
#+LATEX: \end{figure}

The next plot, (ref), shows a large-scale simulation of the Ligurian
earthquake on Ondes3D. This experiment was conducted to put container
technologies in a highly-distributed computing scenario, and its main
objective is to assess the aggregated overhead of spawning a large
number of containers across multiple nodes. Unfortunately, the
container infrastructure for Docker using its overlay network and
Docker Swarm as an orchestrator failed to spawn containers in such a
high number of nodes, and thus Docker was excluded from this test
case. As the plot indicates, there is no observable difference in
execution time between the two approaches (Singularity and Native),
which indicates that the additional cost of executing applications in
a Singularity environment is negligible even when spawning a high
number of containers.

#+LATEX: \begin{figure}[h]
#+LATEX: \centering
#+LATEX: \includegraphics[width=.8\textwidth]{./img/ondes3d-ligurian.png}
#+LATEX: \caption{Execution time for the simulation of the Ligurian earthquake using Ondes3D}
#+LATEX: \end{figure}

To illustrate the advantages in flexibility for environment
configuration, we also conducted an experiment running an Alpine Linux
image on the container environments. The Alpine Linux is a lightweight
Linux distribution that strives for efficiency and security. It is
based on Busybox and provides an alternative set of standard libraries
that can yield better performance in some applications. Although
installing a completely different Linux distribution on multiple hosts
for a single experiment is not feasible (especially in a shared
cluster environment), it can be easily done when using containers. The
plot (ref) shows how Docker and Singularity (running the Alpine Linux
distribution) compare to the native operating system (running
Debian). These results show that, by modifying the execution
environment, it is possible for the virtualized execution to
outperform the native one.

#+LATEX: \begin{figure}[h]
#+LATEX: \centering
#+LATEX: \includegraphics[width=.8\textwidth]{./img/ep-b-alpine.png}
#+LATEX: \caption{Execution time for the NAS EP benchmark with containers running Alpine Linux and the host running Debian}
#+LATEX: \end{figure}

*** EP experiment plot                                           :noexport:
#+begin_src R :results output graphics :file img/ep-b.png :width 600 :height 400
  library(tidyverse)
  
  results <- read_csv('./results/nas/results.csv')
  results <- results %>%
    mutate(time = time/1000) %>%
    group_by(environment, parallelism) %>%
    summarize(
      samples = n(),
      average = mean(time),
      stdDeviation = sd(time),
      stdError = 3*stdDeviation/sqrt(samples)
    )
  results

  custom_theme <- function() {
    ret <- list();
    ret[[length(ret)+1]] <- theme (
      plot.margin = unit(c(0,0,0,0), "cm"),
      legend.spacing = unit(1, "mm"),
      legend.position = "top",
      legend.justification = "left",
      legend.box.spacing = unit(0, "pt"),
      legend.box.margin = margin(0,0,0,0),
      legend.title = element_blank());
    return(ret);
  }

  ggplot(results, aes(x = parallelism, y = average)) +
    scale_x_continuous(breaks = c(1, 4, 8, 16), trans = 'sqrt') +
    ylim(0, NA) +
    geom_point(aes(col = environment), size = 2) +
    geom_line(aes(col = environment), size = 1, alpha = 0.3) + 
    geom_errorbar(aes(ymin = average - stdError, ymax = average + stdError, col = environment), width = 0.2) +
    scale_color_grey() + 
    xlab('Amount of computing units (count)') + 
    ylab('Execution time (s)') +
    theme_bw(base_size = 12) +
    theme(legend.position = 'top', legend.spacing = unit(x = c(0, 0, 0, 0), units = 'mm')) +
    custom_theme()
#+end_src

#+RESULTS:
[[file:img/ep-b.png]]

*** Ondes3D ESSAI experiment plot                                :noexport:
#+begin_src R
library(tidyverse);

results <- read_csv('./results/ondes3d/results.csv');

results <- results %>%
  mutate(time = time/1000) %>%
  group_by(environment, parallelism) %>%
  summarize(
    samples = n(),
    average = mean(time),
    stdDeviation = sd(time),
    stdError = 3*stdDeviation/sqrt(samples)
  );

custom_theme <- function() {
  ret <- list();
  ret[[length(ret)+1]] <- theme (
    plot.margin = unit(c(0,0,0,0), "cm"),
    legend.spacing = unit(1, "mm"),
    legend.position = "top",
    legend.justification = "left",
    legend.box.spacing = unit(0, "pt"),
    legend.box.margin = margin(0,0,0,0),
    legend.title = element_blank());
  return(ret);
}

ggplot(results, aes(x = parallelism, y = average)) + 
  geom_line(aes(col=environment), size = 0.5, alpha=0.2) + 
  geom_point(aes(col=environment), size=2) + 
  geom_errorbar(aes(ymin=average-stdError, ymax=average+stdError, col=environment), width=0.15) +
  scale_color_grey() +
  ylim(0, NA) +
  scale_x_continuous(breaks=c(1, 4, 8, 16), trans='sqrt') + 
  xlab("Amount of computing units (count)") +
  ylab("Execution time (s)") +
  theme_bw(base_size=12) +
  theme(legend.position = "top", legend.spacing = unit(x=c(0,0,0,0),units="mm")) +
  custom_theme();
#+end_src

*** Ping Pong plot                                               :noexport:
#+begin_src R
library(tidyverse)

results <- read_csv('./results/ping-pong/results.csv')
results <- results %>% 
  group_by(environment, size) %>%
  summarize(
    samples = n(),
    average = mean(time),
    stdDeviation = sd(time),
    stdError = 3*stdDeviation/sqrt(samples))

default_theme <- function() {
  ret <- list();
  ret[[length(ret)+1]] <- theme (
    plot.margin = unit(c(0,0,0,0), "cm"),
    legend.spacing = unit(1, "mm"),
    legend.position = "top",
    legend.justification = "left",
    legend.box.spacing = unit(0, "pt"),
    legend.box.margin = margin(0,0,0,0),
    legend.title = element_blank());
  return(ret);
}

ggplot(results,aes(x=size, y=average)) +
  geom_line(aes(col = environment), alpha = 0.2) +
  geom_point(aes(col = environment), size = 3) +
  geom_errorbar(aes(ymin=average-stdError, ymax=average+stdError, color=environment, group=environment), width = 0.3) +
  theme_bw(base_size=12) +
  scale_y_continuous(trans='log2') + 
  #ylim(0,NA) +
  scale_x_continuous(trans="log2") + 
  ylab('Average latency (ms)') +
  xlab('Message size (bytes)') +
  scale_color_grey() +
  default_theme()
#+end_src

*** Ondes3D Ligurian plot                                        :noexport:
#+begin_src R
library(tidyverse);

results <- read_csv('./results/ondes3d-ligurian/results.csv');
results <- results %>%
  mutate(time = time/1000) %>%
  group_by(environment, parallelism) %>%
  summarize(
    samples = n(),
    average = mean(time),
    stdDeviation = sd(time),
    stdError = 3*stdDeviation/sqrt(samples)
  );

default_theme <- function() {
  ret <- list();
  ret[[length(ret)+1]] <- theme (
    plot.margin = unit(c(0,0,0,0), "cm"),
    legend.spacing = unit(1, "mm"),
    legend.position = "top",
    legend.justification = "left",
    legend.box.spacing = unit(0, "pt"),
    legend.box.margin = margin(0,0,0,0),
    legend.title = element_blank());
  return(ret);
}

ggplot(results, aes(x = parallelism, y = average)) + 
  geom_line(aes(col=environment), size = 0.5, alpha=0.2) + 
  geom_point(aes(col=environment), size=2) + 
  geom_errorbar(aes(ymin=average-stdError, ymax=average+stdError, col=environment), width=20) +
  scale_color_grey() +
  scale_x_continuous(breaks=seq(64,256,64)) +
  ylim(0, NA) +
  xlab("Amount of computing units (count)") +
  ylab("Execution time (s)") +
  theme_bw(base_size=12) +
  theme(legend.position = "top", legend.spacing = unit(x=c(0,0,0,0),units="mm")) +
  default_theme();
#+end_src

#+RESULTS:

** Conclusion

In this paper, we discussed the use of virtualization technologies in
the form of Linux containers to help to solve problems such as
reproducibility and user control over HPC environments. Containers
provide similar features as hardware level virtualization with a
theoretically negligible performance overhead, making them suitable
for high-performance applications. In that context, we compared and
contrasted two container technologies, Docker and Singularity, against
a native environment running with no virtualization.

The results for the proposed tests indicate that containers introduce
very little (if any) computational overhead in applications, for both
Docker and Singularity. This can be verified by the lack of a clear
performance difference on the EP NAS Benchmark, in favor of a specific
environment.

Communication overhead, on the other hand, has been observed in Docker
containers. This is mainly because the Docker architecture requires
the containers to be connected through an overlay network in order for
them to have connectivity across multiple hosts (which was needed for
the MPI cluster). This overhead was observed in both the Ping Pong
test case as well as the Ondes3D application, which is known to
require frequent communication between MPI processes. The same
communication overhead could not be observed, however, in Singularity
containers.

Additionally, we conducted experiments that leveraged the potential
flexibility that a virtualized workflow provides. Because containers
allow users to fine-tune the execution environment more easily, it was
possible to use a different Linux distribution without having root
access to the host operating system. This approach yielded better
performance than the native execution, which means that it is possible
to use these fine-tuning capabilities to considerably enhance the
performance of HPC applications.

With the experiments conducted in this research, we can conclude that
Linux containers are a suitable option for running HPC applications in
a virtualized environment, without the drawbacks of traditional
hardware-level virtualization. In our tests, we concluded that
Singularity containers are the most suitable option both in terms of
system administration (for not granting every user that starts a
container root access to the system) and in terms of performance (for
not imposing an overlay network that is a potential bottleneck).

** Acknowledgments                                                  :ignore:

#+LATEX:\section*{Acknowledgements}

We thank these projects for supporting this investigation: FAPERGS
GreenCloud (16/488-9), the FAPERGS MultiGPU (16/354-8), the CNPq
447311/2014-0, the CAPES/Brafitec EcoSud 182/15, and the CAPES/Cofecub
899/18. Experiments were carried out at the Grid'5000 platform
#+Latex: ({\texttt{https://www.grid5000.fr}}),
with support from Inria, CNRS, RENATER and several other french
organizations. The companion material is hosted by CERN's Zenodo for
which we are also grateful.

** References                                                        :ignore:

#+BEGIN_COMMENT LUCAS
You have too few references. You need to more or less triple that
number to more related and context work.
#+END_COMMENT

# See next section to understand how refs.bib file is created.

#+LATEX: \bibliographystyle{sbc}
#+LATEX: \bibliography{refs}

* Bib file is here                                                 :noexport:

Tangle this file with C-c C-v t

#+begin_src bib :tangle refs.bib
@book{jain1991art,
  added-at = {2011-04-19T00:00:00.000+0200},
  author = {Jain, Raj},
  biburl = {https://www.bibsonomy.org/bibtex/2fec32c393af648375f02cde200e33b99/dblp},
  interhash = {655d52a18a388bbf814d7d5e5df1f018},
  intrahash = {fec32c393af648375f02cde200e33b99},
  isbn = {978-0-471-50336-1},
  keywords = {dblp},
  pages = {I-XXVII, 1-685},
  publisher = {Wiley},
  series = {Wiley professional computing},
  timestamp = {2011-04-29T15:27:00.000+0200},
  title = {The art of computer systems performance analysis - techniques for experimental design, measurement, simulation, and modeling.},
  year = 1991
}

@inproceedings{tesser2017using,
  title={Using Simulation to Evaluate and Tune the Performance of Dynamic Load Balancing of an Over-decomposed Geophysics Application},
  author={Tesser, Rafael Keller and Schnorr, Lucas Mello and Legrand, Arnaud and Dupros, Fabrice and Navaux, Philippe Olivier Alexandre},
  booktitle={European Conference on Parallel Processing},
  pages={192--205},
  year={2017},
  organization={Springer}
}

@article{dupros:10,
title = "High-performance finite-element simulations of seismic wave propagation in three-dimensional nonlinear inelastic geological media",
journal = "Parallel Comput",
volume = "36",
number = "5",
pages = "308 - 325",
year = "2010",
issn = "0167-8191",
author = "Fabrice Dupros and Florent De Martin and Evelyne Foerster and Dimitri Komatitsch and Jean Roman",
keywords = "Seismic numerical simulation, Finite-element method, Parallel sparse direct solver, Nonlinear soil behaviour"
}

@article{bailey1991parallel,
  title={The NAS parallel benchmarks},
  author={Bailey, David H and Barszcz, Eric and Barton, John T and Browning, David S and Carter, Robert L and Dagum, Leonardo and Fatoohi, Rod A and Frederickson, Paul O and Lasinski, Thomas A and Schreiber, Rob S and others},
  journal={The International Journal of Supercomputing Applications},
  volume={5},
  number={3},
  pages={63--73},
  year={1991},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@article{jeanvoine2013kadeploy3,
  title={Kadeploy3: Efficient and scalable operating system provisioning for clusters},
  author={Jeanvoine, Emmanuel and Sarzyniec, Luc and Nussbaum, Lucas},
  journal={USENIX; login:},
  volume={38},
  number={1},
  pages={38--44},
  year={2013}
}

@incollection{grid5000,
   title = {Adding Virtualization Capabilities to the {Grid'5000} Testbed},
   author = {Balouek, Daniel and Carpen Amarie, Alexandra and Charrier, Ghislain and Desprez, Fr{\'e}d{\'e}ric and Jeannot, Emmanuel and Jeanvoine, Emmanuel and L{\`e}bre, Adrien and Margery, David and Niclausse, Nicolas and Nussbaum, Lucas and Richard, Olivier and P{\'e}rez, Christian and Quesnel, Flavien and Rohr, Cyril and Sarzyniec, Luc},
   booktitle = {Cloud Computing and Services Science},
   publisher = {Springer International Publishing},
   pages = {3-20},
   volume = {367},
   editor = {Ivanov, Ivan I. and van Sinderen, Marten and Leymann, Frank and Shan, Tony },
   series = {Communications in Computer and Information Science },
   isbn = {978-3-319-04518-4 },
   doi = {10.1007/978-3-319-04519-1\_1 },
   year = {2013},
}

@INPROCEEDINGS{7562612, 
    author={M. T. Chung and N. Quang-Hung and M. T. Nguyen and N. Thoai}, 
    booktitle={2016 IEEE Sixth International Conference on Communications and Electronics (ICCE)}, 
    title={Using Docker in high performance computing applications}, 
    year={2016}, 
    volume={}, 
    number={}, 
    pages={52-57}, 
    keywords={cloud computing;data handling;parallel processing;virtual machines;virtualisation;Docker;HPC;VM;cloud computing;data intensive application;high performance computing;resource management;virtual machines;virtualization technology;Cloud computing;Computer architecture;Containers;Libraries;Virtual machine monitors;Virtual machining;Virtualization;Docker;Graph500;HPC;HPL;cloud computing;performance evaluation}, 
    doi={10.1109/CCE.2016.7562612}, 
    ISSN={}, 
    month={July}
}

@INPROCEEDINGS{7868429, 
    author={N. Nguyen and D. Bein}, 
    booktitle={2017 IEEE 7th Annual Computing and Communication Workshop and Conference (CCWC)}, 
    title={Distributed MPI cluster with Docker Swarm mode}, 
    year={2017}, 
    volume={}, 
    number={}, 
    pages={1-7}, 
    keywords={application program interfaces;containerisation;message passing;parallel processing;source code (software);MPI programs;container orchestration technology;distributed MPI cluster;docker swarm mode;high-performance computing;modern containerization technology;source code;Cloud computing;Computers;Containers;File systems;Linux;Operating systems;Cluster Automation;Container;Distributed System;Docker;Docker Swarm mode;HPC;MPI}, 
    doi={10.1109/CCWC.2017.7868429}, 
    ISSN={}, 
    month={Jan}
}

@article{10.1371/journal.pone.0177459,
    author = {Kurtzer, Gregory M. AND Sochat, Vanessa AND Bauer, Michael W.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Singularity: Scientific containers for mobility of compute},
    year = {2017},
    month = {05},
    volume = {12},
    url = {https://doi.org/10.1371/journal.pone.0177459},
    pages = {1-20},
    abstract = {Here we present Singularity, software developed to bring containers and reproducibility to scientific computing. Using Singularity containers, developers can work in reproducible environments of their choosing and design, and these complete environments can easily be copied and executed on other platforms. Singularity is an open source initiative that harnesses the expertise of system and software engineers and researchers alike, and integrates seamlessly into common workflows for both of these groups. As its primary use case, Singularity brings mobility of computing to both users and HPC centers, providing a secure means to capture and distribute software and compute environments. This ability to create and deploy reproducible environments across these centers, a previously unmet need, makes Singularity a game changing development for computational science.},
    number = {5},
    doi = {10.1371/journal.pone.0177459}
}

@inproceedings{Le:2017:PAA:3093338.3106737,
    author = {Le, Emily and Paz, David},
    title = {Performance Analysis of Applications Using Singularity Container on SDSC Comet},
    booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact},
    series = {PEARC17},
    year = {2017},
    isbn = {978-1-4503-5272-7},
    location = {New Orleans, LA, USA},
    pages = {66:1--66:4},
    articleno = {66},
    numpages = {4},
    url = {http://doi.acm.org/10.1145/3093338.3106737},
    doi = {10.1145/3093338.3106737},
    acmid = {3106737},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {IMB: Intel's MPI Benchmark, NEURON: Neuronal Simulation Tool, OSU: Ohio State University Benchmark, Singularity},
}
#+end_src
* Emacs setup                                                      :noexport:

# Local Variables:
# eval: (add-to-list 'load-path ".")
# eval: (require 'ox-extra)
# eval: (ox-extras-activate '(ignore-headlines))
# eval: (add-to-list 'org-latex-classes '("article" "\\documentclass{article}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval: (setq org-latex-to-pdf-process '("pdflatex -interaction nonstopmode -output-directory %o %f ; bibtex `basename %f | sed 's/\.tex//'` ; pdflatex -interaction nonstopmode -output-directory  %o %f ; pdflatex -interaction nonstopmode -output-directory %o %f"))
# eval: (setq ispell-local-dictionary "american")
# eval: (eval (flyspell-mode t))
# eval: (setq org-latex-with-hyperref nil)
# End:
